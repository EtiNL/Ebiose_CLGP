embed_dim: 512

graph_encoder:
  name: 'GCN'
  hidden: 4

text_encoder:
  name: 'Transformer'
  layers: 12
  heads: 8
  width: 512
  feedforward_dim: 2048
  max_position_embeddings: 512
  activation_function: 'gelu'
  layer_norm_eps: 1e-12
  initializer_range: 0.02