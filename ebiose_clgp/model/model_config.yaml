embed_dim: 512

graph_encoder:
  name: 'GCN'
  hidden: 4

text_encoder:
  name: 'Transformer'
  layers: 4
  heads: 2
  width: 512
  feedforward_dim: 1028
  max_position_embeddings: 1000
  activation_function: 'gelu'
  layer_norm_eps: 1e-12
  initializer_range: 0.02